{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch #Main library\n",
    "from torch.autograd import Variable # To create Tensors\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader # To create data entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1050 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0) # See you GPU name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([[i , j] for i, j in zip(range(100), range(1, 101))])).cuda() # .cuda() puts the tensor on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # See the shape of your tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data[:10,1] # Access the values of your tenosr by .data and slicing like numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = Variable(torch.Tensor([[2*i+3*j] for i, j in zip(range(100), range(1, 101))])).cuda() # Create an arbitrary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some models to predict y from x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model(torch.nn.Module): # To create a model just extend nn.Module and implement __init__ and forward methods\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.lin = torch.nn.Linear(2, 1) # in __init__ you define the elements of the network\n",
    "    def forward(self, x):\n",
    "        y = self.lin(x) # in forward just pass the input through the element you created in __init__\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model() # instantiate you model\n",
    "m.cuda() # put it on GPU\n",
    "criterion = torch.nn.MSELoss() # define a loss \n",
    "optim = torch.optim.Adam(m.parameters(),0.2) # pass the parameters of your model to an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss:  97722.359375\n",
      "epoch:  100 loss:  1.993674635887146\n",
      "epoch:  200 loss:  1.5573391914367676\n",
      "epoch:  300 loss:  1.1968315839767456\n",
      "epoch:  400 loss:  0.8683893084526062\n",
      "epoch:  500 loss:  0.5970905423164368\n",
      "epoch:  600 loss:  0.3897525668144226\n",
      "epoch:  700 loss:  0.24170124530792236\n",
      "epoch:  800 loss:  0.14239442348480225\n",
      "epoch:  900 loss:  0.07965555042028427\n",
      "epoch:  1000 loss:  0.042272165417671204\n",
      "epoch:  1100 loss:  0.021254301071166992\n",
      "epoch:  1200 loss:  0.010108155198395252\n",
      "epoch:  1300 loss:  0.004539478570222855\n",
      "epoch:  1400 loss:  0.0019210150931030512\n",
      "epoch:  1500 loss:  0.0007641716510988772\n",
      "epoch:  1600 loss:  0.0002848840958904475\n",
      "epoch:  1700 loss:  9.940329618984833e-05\n",
      "epoch:  1800 loss:  3.2302294130204245e-05\n",
      "epoch:  1900 loss:  9.733891602081712e-06\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000): # define your training loop\n",
    "    y_ = m(x) # pass the tensor you created before through the model\n",
    "    loss = criterion(y_, y) # calculate the loss of the prediction from actual\n",
    "    if i%100 ==0:\n",
    "        print('epoch: ',i, 'loss: ', loss.item()) # .item is used with tensors with rank 0 i.e. scalers\n",
    "    optim.zero_grad() # reset any gradient calculations if any\n",
    "    loss.backward() # .backward() calculates the gradient for all the parameters involved in calculating the loss\n",
    "    optim.step() # finally do the gradient descent update\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Datasets are there to conveniently fetch data for training. To define them you have to implement 3 methods as shown below\n",
    "class custom_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.Tensor([[i , j] for i, j in zip(range(100), range(1, 101))])\n",
    "        self.y = torch.Tensor([[2*i+3*j] for i, j in zip(range(100), range(1, 101))])\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "dataset = custom_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dloader = DataLoader(dataset, 10, True) # Dataloader is built on datasets to fascilitate fetching data as you will see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = model() # instantiate you model\n",
    "m.cuda() # put it on GPU\n",
    "criterion = torch.nn.MSELoss() # define a loss \n",
    "optim = torch.optim.Adam(m.parameters(),0.2) # pass the parameters of your model to an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0 loss:  82786.875\n",
      "iter:  50 loss:  388.1860046386719\n",
      "iter:  100 loss:  1.9986851215362549\n",
      "iter:  150 loss:  0.30013787746429443\n",
      "iter:  200 loss:  0.633473813533783\n",
      "iter:  250 loss:  0.3923751413822174\n",
      "iter:  300 loss:  0.6905971169471741\n",
      "iter:  350 loss:  0.16636331379413605\n",
      "iter:  400 loss:  0.3838123381137848\n",
      "iter:  450 loss:  0.28431567549705505\n",
      "iter:  500 loss:  0.3048263490200043\n",
      "iter:  550 loss:  0.05782446265220642\n",
      "iter:  600 loss:  0.06819303333759308\n",
      "iter:  650 loss:  0.15540829300880432\n",
      "iter:  700 loss:  0.05706700682640076\n",
      "iter:  750 loss:  0.04764743521809578\n",
      "iter:  800 loss:  0.0459921658039093\n",
      "iter:  850 loss:  0.024443689733743668\n",
      "iter:  900 loss:  0.015892446041107178\n",
      "iter:  950 loss:  0.02069912850856781\n",
      "iter:  1000 loss:  0.015077618882060051\n",
      "iter:  1050 loss:  0.005280360579490662\n",
      "iter:  1100 loss:  0.006497129797935486\n",
      "iter:  1150 loss:  0.0035856044851243496\n",
      "iter:  1200 loss:  0.002094197552651167\n",
      "iter:  1250 loss:  0.0011256985599175096\n",
      "iter:  1300 loss:  0.0013101191725581884\n",
      "iter:  1350 loss:  0.0006042360328137875\n",
      "iter:  1400 loss:  0.0002027744922088459\n",
      "iter:  1450 loss:  0.0001564656849950552\n",
      "iter:  1500 loss:  0.00015117884322535247\n",
      "iter:  1550 loss:  7.398443267447874e-05\n",
      "iter:  1600 loss:  5.0828974053729326e-05\n",
      "iter:  1650 loss:  1.9757218979066238e-05\n",
      "iter:  1700 loss:  1.3964809113531373e-05\n",
      "iter:  1750 loss:  6.0328807194309775e-06\n",
      "iter:  1800 loss:  3.9427250158041716e-06\n",
      "iter:  1850 loss:  2.055522827504319e-06\n",
      "iter:  1900 loss:  1.2006727274638251e-06\n",
      "iter:  1950 loss:  4.2970060576408287e-07\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    for j, data  in enumerate(dloader, 0): # This is very useful when you are using mini bathces\n",
    "        x, y = data\n",
    "        x, y = Variable(x).cuda(), Variable(y).cuda()\n",
    "        y_ = m(x)\n",
    "        loss = criterion(y_, y)\n",
    "        if (10*i+j) %50 ==0:\n",
    "            print('iter: ', 10*i+j, 'loss: ', loss.item())\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a model on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's use built-in MNISt dataset from torch bu using the following syntax. We also make normalization transformation.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./mnist_data', train=True, download=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./mnist_data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F # To access activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(784, 520)\n",
    "        self.l2 = torch.nn.Linear(520, 320)\n",
    "        self.l3 = torch.nn.Linear(320, 240)\n",
    "        self.l4 = torch.nn.Linear(240, 120)\n",
    "        self.l5 = torch.nn.Linear(120, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) # .view help us view a tensor in desired shape which is a vector 28x28\n",
    "        x = F.relu(self.l1(x)) # between layers we apply activation\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = self.l5(x) # Softmax is not needed as we use nn.CrossEntropyLoss()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = model()\n",
    "m.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss() # This is the loss for multi-class classification\n",
    "optim = torch.optim.SGD(m.parameters(), 0.01, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0 loss:  2.297628164291382\n",
      "iter:  100 loss:  2.278578281402588\n",
      "iter:  200 loss:  2.1996800899505615\n",
      "iter:  300 loss:  1.76529860496521\n",
      "iter:  400 loss:  0.811119019985199\n",
      "iter:  500 loss:  0.4767480790615082\n",
      "iter:  600 loss:  0.366688072681427\n",
      "iter:  700 loss:  0.5400040149688721\n",
      "iter:  800 loss:  0.6790627837181091\n",
      "iter:  900 loss:  0.40882009267807007\n",
      "iter:  1000 loss:  0.36237797141075134\n",
      "iter:  1100 loss:  0.15433190762996674\n",
      "iter:  1200 loss:  0.25847288966178894\n",
      "iter:  1300 loss:  0.18450406193733215\n",
      "iter:  1400 loss:  0.2636382579803467\n",
      "iter:  1500 loss:  0.17533031105995178\n",
      "iter:  1600 loss:  0.2104531079530716\n",
      "iter:  1700 loss:  0.3601965308189392\n",
      "iter:  1800 loss:  0.10654174536466599\n",
      "iter:  1900 loss:  0.11978012323379517\n",
      "iter:  2000 loss:  0.2898464500904083\n",
      "iter:  2100 loss:  0.16514664888381958\n",
      "iter:  2200 loss:  0.1471841037273407\n",
      "iter:  2300 loss:  0.05693632364273071\n",
      "iter:  2400 loss:  0.1773693561553955\n",
      "iter:  2500 loss:  0.21616941690444946\n",
      "iter:  2600 loss:  0.19267240166664124\n",
      "iter:  2700 loss:  0.17630702257156372\n",
      "iter:  2800 loss:  0.0809619277715683\n",
      "iter:  2900 loss:  0.21952968835830688\n",
      "iter:  3000 loss:  0.10538257658481598\n",
      "iter:  3100 loss:  0.09774886071681976\n",
      "iter:  3200 loss:  0.22700396180152893\n",
      "iter:  3300 loss:  0.20725668966770172\n",
      "iter:  3400 loss:  0.11154359579086304\n",
      "iter:  3500 loss:  0.07283107191324234\n",
      "iter:  3600 loss:  0.05601031333208084\n",
      "iter:  3700 loss:  0.030124589800834656\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(train_loader)\n",
    "for i in range(4):\n",
    "    for j, (x, y)  in enumerate(train_loader, 0):\n",
    "        x, y = Variable(x).cuda(), Variable(y).cuda()\n",
    "        y_ = m(x)\n",
    "        loss = criterion(y_, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (i*n_batches+j)%100 ==0:\n",
    "            print('iter: ', n_batches*i+j, 'loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for (x, y) in test_loader:\n",
    "    x, y = Variable(x).cuda(), Variable(y).cuda()\n",
    "    y_=m(x)\n",
    "    pred = torch.max(y_.data, 1)[1] # The 10 outputs for eacn input indicate the likelihood of the classes so we take the max\n",
    "    correct += pred.eq(y.data.view_as(pred)).cpu().sum() # Bring the data on CPU to use with numpy\n",
    "print(correct.numpy()/len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model(torch.nn.Module): # This is not much different from previous procedures\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.c1 = torch.nn.Conv2d(1, 10, 5)\n",
    "        self.c2 = torch.nn.Conv2d(10, 20, 5)\n",
    "        self.mp = torch.nn.MaxPool2d(2)\n",
    "        self.l1 = torch.nn.Linear(2000, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.c1(x))\n",
    "        x = F.relu(self.c2(x))\n",
    "        x = F.relu(self.mp(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = model()\n",
    "m.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(m.parameters(), 0.01, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0 loss:  2.318005323410034\n",
      "iter:  100 loss:  0.49953484535217285\n",
      "iter:  200 loss:  0.3001226782798767\n",
      "iter:  300 loss:  0.20771130919456482\n",
      "iter:  400 loss:  0.06426689773797989\n",
      "iter:  500 loss:  0.3679344356060028\n",
      "iter:  600 loss:  0.08783812820911407\n",
      "iter:  700 loss:  0.18321630358695984\n",
      "iter:  800 loss:  0.05472637712955475\n",
      "iter:  900 loss:  0.07104971259832382\n",
      "iter:  1000 loss:  0.031181804835796356\n",
      "iter:  1100 loss:  0.08725129812955856\n",
      "iter:  1200 loss:  0.13378097116947174\n",
      "iter:  1300 loss:  0.07342841476202011\n",
      "iter:  1400 loss:  0.034585341811180115\n",
      "iter:  1500 loss:  0.02790471911430359\n",
      "iter:  1600 loss:  0.04863227158784866\n",
      "iter:  1700 loss:  0.02295522391796112\n",
      "iter:  1800 loss:  0.045062221586704254\n",
      "iter:  1900 loss:  0.07014953345060349\n",
      "iter:  2000 loss:  0.016720734536647797\n",
      "iter:  2100 loss:  0.025574102997779846\n",
      "iter:  2200 loss:  0.020966418087482452\n",
      "iter:  2300 loss:  0.042456962168216705\n",
      "iter:  2400 loss:  0.01690921187400818\n",
      "iter:  2500 loss:  0.014461919665336609\n",
      "iter:  2600 loss:  0.043225206434726715\n",
      "iter:  2700 loss:  0.04924074560403824\n",
      "iter:  2800 loss:  0.03094957023859024\n",
      "iter:  2900 loss:  0.043303728103637695\n",
      "iter:  3000 loss:  0.008160240948200226\n",
      "iter:  3100 loss:  0.08765514940023422\n",
      "iter:  3200 loss:  0.03784462809562683\n",
      "iter:  3300 loss:  0.0432327464222908\n",
      "iter:  3400 loss:  0.04588642716407776\n",
      "iter:  3500 loss:  0.032609179615974426\n",
      "iter:  3600 loss:  0.061041638255119324\n",
      "iter:  3700 loss:  0.023587077856063843\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(train_loader)\n",
    "for i in range(4):\n",
    "    for j, (x, y)  in enumerate(train_loader, 0):\n",
    "        x, y = Variable(x).cuda(), Variable(y).cuda()\n",
    "        y_ = m(x)\n",
    "        loss = criterion(y_, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (i*n_batches+j)%100 ==0:\n",
    "            print('iter: ', n_batches*i+j, 'loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for (x, y) in test_loader:\n",
    "    x, y = Variable(x).cuda(), Variable(y).cuda()\n",
    "    y_=m(x)\n",
    "    pred = torch.max(y_.data, 1)[1]\n",
    "    correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "print(correct.numpy()/len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now shift our focus on sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets teach the network to say 'hihello', by teaching it to predict the next character after each input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_char = list('hielo')\n",
    "char_idx = {char:i for i, char in enumerate(idx_char)}\n",
    "def lookup(idx):\n",
    "    out = [0]*num_classes\n",
    "    out[idx] = 1\n",
    "    return out\n",
    "x = torch.Tensor([[lookup(char_idx[char]) for char in 'hihell']])\n",
    "y = torch.LongTensor([char_idx[char] for char in 'ihello'])# the target is the input shifted one place to the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "hidden_size = 5 # arbitrary but good choice is the alphabet cardinality\n",
    "num_layers = 1\n",
    "input_size = 5 # one hot vector of input has dim 5\n",
    "num_classes = 5 # we have 5 classes: h i e l o\n",
    "sequence_len = 6 # The input sequence has 6 chars\n",
    "class model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.rnn = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        x = x.view(batch_size, sequence_len, input_size)\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = out.view(-1, num_classes) # this predict the next char\n",
    "        return hidden, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = model()\n",
    "m.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(m.parameters(), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1.6514673233032227 output: iiiiii\n",
      "epoch: 1 loss: 1.5523813962936401 output: iiiiii\n",
      "epoch: 2 loss: 1.4848428964614868 output: iiilll\n",
      "epoch: 3 loss: 1.421666145324707 output: illlll\n",
      "epoch: 4 loss: 1.354124903678894 output: ehelll\n",
      "epoch: 5 loss: 1.2811726331710815 output: ehelll\n",
      "epoch: 6 loss: 1.2022839784622192 output: ehelll\n",
      "epoch: 7 loss: 1.1221809387207031 output: ehelll\n",
      "epoch: 8 loss: 1.0482736825942993 output: ehelll\n",
      "epoch: 9 loss: 0.9841535091400146 output: ehelll\n",
      "epoch: 10 loss: 0.9335740208625793 output: ehelll\n",
      "epoch: 11 loss: 0.8924670815467834 output: ihelll\n",
      "epoch: 12 loss: 0.8565268516540527 output: ihelll\n",
      "epoch: 13 loss: 0.8272908329963684 output: ihelll\n",
      "epoch: 14 loss: 0.8011173605918884 output: ihelll\n",
      "epoch: 15 loss: 0.7755684852600098 output: ihelll\n",
      "epoch: 16 loss: 0.7538593411445618 output: ihelll\n",
      "epoch: 17 loss: 0.7384834885597229 output: ihelll\n",
      "epoch: 18 loss: 0.7260566353797913 output: ihelll\n",
      "epoch: 19 loss: 0.715537965297699 output: ihelll\n",
      "epoch: 20 loss: 0.7073485851287842 output: ihelll\n",
      "epoch: 21 loss: 0.7003188729286194 output: ihelll\n",
      "epoch: 22 loss: 0.6934380531311035 output: ihelll\n",
      "epoch: 23 loss: 0.6864304542541504 output: ihelll\n",
      "epoch: 24 loss: 0.6792013049125671 output: ihello\n",
      "epoch: 25 loss: 0.6721840500831604 output: ihello\n",
      "epoch: 26 loss: 0.6674268245697021 output: ihello\n",
      "epoch: 27 loss: 0.6658380627632141 output: ihello\n",
      "epoch: 28 loss: 0.6648913025856018 output: ihello\n",
      "epoch: 29 loss: 0.6642266511917114 output: ihello\n",
      "epoch: 30 loss: 0.6639792323112488 output: ihello\n",
      "epoch: 31 loss: 0.661164402961731 output: ihello\n",
      "epoch: 32 loss: 0.6578716039657593 output: ihello\n",
      "epoch: 33 loss: 0.6551526188850403 output: ihello\n",
      "epoch: 34 loss: 0.6526274085044861 output: ihello\n",
      "epoch: 35 loss: 0.6511123180389404 output: ihello\n",
      "epoch: 36 loss: 0.6501162648200989 output: ihello\n",
      "epoch: 37 loss: 0.6494771838188171 output: ihello\n",
      "epoch: 38 loss: 0.6488029360771179 output: ihello\n",
      "epoch: 39 loss: 0.6475934386253357 output: ihello\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    x, y = Variable(x).cuda(), Variable(y).cuda()\n",
    "    hidden, y_ = m(x)\n",
    "    loss = criterion(y_, y)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    _, outputs = y_.max(1)\n",
    "    outputs = outputs.cpu().numpy()\n",
    "    result = [idx_char[idx] for idx in outputs.squeeze()]\n",
    "    if i%1 ==0:\n",
    "        print('epoch:', i, 'loss:', loss.item(), 'output:', ''.join(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see how we can use embedding with previous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "hidden_size = 5\n",
    "num_layers = 1\n",
    "num_classes = 5\n",
    "sequence_len = 6\n",
    "embed_dim = 4 # choose an aribtrary embedding dim\n",
    "class model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(num_classes, embed_dim,\n",
    "                                        _weight=torch.Tensor(np.eye(num_classes, embed_dim)))# initialize it with one-hot\n",
    "        self.rnn = torch.nn.LSTM(input_size=embed_dim, hidden_size=hidden_size, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        x = x.view(batch_size, sequence_len)\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return hidden, out\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, num_layers, batch_size, hidden_size)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = model()\n",
    "m.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(m.parameters(), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_char = list('hielo')\n",
    "char_idx = {char:i for i, char in enumerate(idx_char)}\n",
    "x = torch.LongTensor([[char_idx[char] for char in 'hihell']])\n",
    "y = torch.LongTensor([char_idx[char] for char in 'ihello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1.6220346689224243 output: oeoooo\n",
      "epoch: 1 loss: 1.512691617012024 output: oooooo\n",
      "epoch: 2 loss: 1.4197088479995728 output: oheooo\n",
      "epoch: 3 loss: 1.329732894897461 output: eheloo\n",
      "epoch: 4 loss: 1.2419105768203735 output: eheloo\n",
      "epoch: 5 loss: 1.1701480150222778 output: eheloo\n",
      "epoch: 6 loss: 1.1195186376571655 output: ehello\n",
      "epoch: 7 loss: 1.0829476118087769 output: ehello\n",
      "epoch: 8 loss: 1.0513219833374023 output: ehello\n",
      "epoch: 9 loss: 1.024389386177063 output: ehelll\n",
      "epoch: 10 loss: 1.0052191019058228 output: ehelll\n",
      "epoch: 11 loss: 0.977078914642334 output: ehelll\n",
      "epoch: 12 loss: 0.9521985650062561 output: ehelll\n",
      "epoch: 13 loss: 0.9315541386604309 output: ehelll\n",
      "epoch: 14 loss: 0.9066872596740723 output: ihelll\n",
      "epoch: 15 loss: 0.8779399991035461 output: ihelll\n",
      "epoch: 16 loss: 0.8530540466308594 output: ihelll\n",
      "epoch: 17 loss: 0.8387728333473206 output: ihelll\n",
      "epoch: 18 loss: 0.8274298310279846 output: ihelll\n",
      "epoch: 19 loss: 0.8180227875709534 output: ihelll\n",
      "epoch: 20 loss: 0.8081479668617249 output: ihelll\n",
      "epoch: 21 loss: 0.7961020469665527 output: ihelll\n",
      "epoch: 22 loss: 0.7848808169364929 output: ihelll\n",
      "epoch: 23 loss: 0.7779327034950256 output: ihelll\n",
      "epoch: 24 loss: 0.7722167372703552 output: ihelll\n",
      "epoch: 25 loss: 0.7651515007019043 output: ihelll\n",
      "epoch: 26 loss: 0.7586175799369812 output: ihelll\n",
      "epoch: 27 loss: 0.7502331733703613 output: ihelll\n",
      "epoch: 28 loss: 0.7372696995735168 output: ihelll\n",
      "epoch: 29 loss: 0.723168671131134 output: ihelll\n",
      "epoch: 30 loss: 0.7126548290252686 output: ihelll\n",
      "epoch: 31 loss: 0.7045072913169861 output: ihelll\n",
      "epoch: 32 loss: 0.6984555125236511 output: ihelll\n",
      "epoch: 33 loss: 0.6931586861610413 output: ihello\n",
      "epoch: 34 loss: 0.6872026920318604 output: ihello\n",
      "epoch: 35 loss: 0.6829732060432434 output: ihello\n",
      "epoch: 36 loss: 0.6818388104438782 output: ihello\n",
      "epoch: 37 loss: 0.6836394667625427 output: ihello\n",
      "epoch: 38 loss: 0.6829309463500977 output: ihello\n",
      "epoch: 39 loss: 0.6773677468299866 output: ihello\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "#     hidden = m.init_hidden()\n",
    "    x, y = Variable(x).cuda(), Variable(y).cuda()\n",
    "    hidden, y_ = m(x)\n",
    "    loss = criterion(y_, y)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    _, outputs = y_.max(1)\n",
    "    outputs = outputs.cpu().numpy()\n",
    "    result = [idx_char[idx] for idx in outputs.squeeze()]\n",
    "    if i%1 ==0:\n",
    "        print('epoch:', i, 'loss:', loss.item(), 'output:', ''.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 2.1113, -0.4399, -2.2628,  0.9665],\n",
       "        [-1.4425,  2.8398, -1.7243,  1.5508],\n",
       "        [ 1.5335, -1.4601,  1.4267,  2.3674],\n",
       "        [ 0.4673, -1.3563, -1.1912,  2.7823],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.embed.weight # the last row of embedding matrix does not change becuase the network never sees o as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Name Country Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we predict which country a name comes from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from name_dataset import NameDataset # import data from helper file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "N_LAYERS = 1\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = NameDataset(is_train_set=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset = NameDataset(is_train_set=True)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 countries\n"
     ]
    }
   ],
   "source": [
    "N_COUNTRIES = len(train_dataset.get_countries())\n",
    "print(N_COUNTRIES, \"countries\")\n",
    "N_CHARS = 128  # ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = int(bidirectional) + 1\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size, 0)\n",
    "        self.lstm = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          bidirectional=bidirectional, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size*self.n_directions, output_size)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "#         lstm_input = pack_padded_sequence(\n",
    "#             embedded, seq_lengths.data.cpu().numpy())\n",
    "\n",
    "        # To compact weights again call flatten_parameters().\n",
    "#         self.lstm.flatten_parameters()\n",
    "        output, hidden = self.lstm(embedded)\n",
    "\n",
    "        # Use the last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(output[:,-1,:])\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (names, countries) in enumerate(train_loader, 1):\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "#         print(input.shape, output.shape)\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        classifier.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
    "                time_since(start), epoch,  i *\n",
    "                len(names), len(train_loader.dataset),\n",
    "                100. * i * len(names) / len(train_loader.dataset),\n",
    "                total_loss / i * len(names)))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Testing cycle\n",
    "def test(name=None):\n",
    "    # Predict for a given name\n",
    "    if name:\n",
    "        input, seq_lengths, target = make_variables([name], [])\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        country_id = pred.cpu().numpy()[0][0]\n",
    "        print(name, \"is\", train_dataset.get_country(country_id))\n",
    "        return\n",
    "\n",
    "    print(\"evaluating trained model ...\")\n",
    "    correct = 0\n",
    "    train_data_size = len(test_loader.dataset)\n",
    "\n",
    "    for names, countries in test_loader:\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, train_data_size, 100. * correct / train_data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def create_variable(tensor):\n",
    "    # Do cuda() before wrapping with variable\n",
    "    if torch.cuda.is_available():\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "# pad sequences and sort the tensor\n",
    "def pad_sequences(vectorized_seqs, seq_lengths, countries):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # Sort tensors by their length\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "    # Also sort the target (countries) in the same order\n",
    "    target = countries2tensor(countries)\n",
    "    if len(countries):\n",
    "        target = target[perm_idx]\n",
    "\n",
    "    # Return variables\n",
    "    # DataParallel requires everything to be a Variable\n",
    "    return create_variable(seq_tensor), \\\n",
    "        create_variable(seq_lengths), \\\n",
    "        create_variable(target)\n",
    "\n",
    "\n",
    "# Create necessary variables, lengths, and target\n",
    "def make_variables(names, countries):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n",
    "\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "\n",
    "def countries2tensor(countries):\n",
    "    country_ids = [train_dataset.get_country_id(\n",
    "        country) for country in countries]\n",
    "    return torch.LongTensor(country_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs...\n",
      "[0m 0s] Train Epoch: 1 [2560/13374 (19%)]\tLoss: 688.69\n",
      "[0m 0s] Train Epoch: 1 [5120/13374 (38%)]\tLoss: 620.57\n",
      "[0m 0s] Train Epoch: 1 [7680/13374 (57%)]\tLoss: 572.29\n",
      "[0m 0s] Train Epoch: 1 [10240/13374 (77%)]\tLoss: 541.51\n",
      "[0m 0s] Train Epoch: 1 [12800/13374 (96%)]\tLoss: 519.51\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 3134/6700 (46%)\n",
      "\n",
      "Sung is Russian\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 0s] Train Epoch: 2 [2560/13374 (19%)]\tLoss: 409.77\n",
      "[0m 0s] Train Epoch: 2 [5120/13374 (38%)]\tLoss: 396.17\n",
      "[0m 1s] Train Epoch: 2 [7680/13374 (57%)]\tLoss: 391.29\n",
      "[0m 1s] Train Epoch: 2 [10240/13374 (77%)]\tLoss: 387.37\n",
      "[0m 1s] Train Epoch: 2 [12800/13374 (96%)]\tLoss: 381.77\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 3839/6700 (57%)\n",
      "\n",
      "Sung is English\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 1s] Train Epoch: 3 [2560/13374 (19%)]\tLoss: 337.51\n",
      "[0m 1s] Train Epoch: 3 [5120/13374 (38%)]\tLoss: 336.26\n",
      "[0m 1s] Train Epoch: 3 [7680/13374 (57%)]\tLoss: 335.07\n",
      "[0m 1s] Train Epoch: 3 [10240/13374 (77%)]\tLoss: 330.64\n",
      "[0m 2s] Train Epoch: 3 [12800/13374 (96%)]\tLoss: 328.59\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 4316/6700 (64%)\n",
      "\n",
      "Sung is German\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 2s] Train Epoch: 4 [2560/13374 (19%)]\tLoss: 296.73\n",
      "[0m 2s] Train Epoch: 4 [5120/13374 (38%)]\tLoss: 299.93\n",
      "[0m 2s] Train Epoch: 4 [7680/13374 (57%)]\tLoss: 297.99\n",
      "[0m 2s] Train Epoch: 4 [10240/13374 (77%)]\tLoss: 292.98\n",
      "[0m 2s] Train Epoch: 4 [12800/13374 (96%)]\tLoss: 290.69\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 4636/6700 (69%)\n",
      "\n",
      "Sung is English\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 3s] Train Epoch: 5 [2560/13374 (19%)]\tLoss: 255.05\n",
      "[0m 3s] Train Epoch: 5 [5120/13374 (38%)]\tLoss: 261.14\n",
      "[0m 3s] Train Epoch: 5 [7680/13374 (57%)]\tLoss: 261.70\n",
      "[0m 3s] Train Epoch: 5 [10240/13374 (77%)]\tLoss: 259.56\n",
      "[0m 3s] Train Epoch: 5 [12800/13374 (96%)]\tLoss: 257.72\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 4826/6700 (72%)\n",
      "\n",
      "Sung is English\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 4s] Train Epoch: 6 [2560/13374 (19%)]\tLoss: 235.57\n",
      "[0m 4s] Train Epoch: 6 [5120/13374 (38%)]\tLoss: 238.31\n",
      "[0m 4s] Train Epoch: 6 [7680/13374 (57%)]\tLoss: 234.88\n",
      "[0m 4s] Train Epoch: 6 [10240/13374 (77%)]\tLoss: 232.23\n",
      "[0m 4s] Train Epoch: 6 [12800/13374 (96%)]\tLoss: 229.62\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5007/6700 (74%)\n",
      "\n",
      "Sung is Irish\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 5s] Train Epoch: 7 [2560/13374 (19%)]\tLoss: 201.63\n",
      "[0m 5s] Train Epoch: 7 [5120/13374 (38%)]\tLoss: 200.81\n",
      "[0m 5s] Train Epoch: 7 [7680/13374 (57%)]\tLoss: 203.00\n",
      "[0m 6s] Train Epoch: 7 [10240/13374 (77%)]\tLoss: 206.51\n",
      "[0m 6s] Train Epoch: 7 [12800/13374 (96%)]\tLoss: 206.84\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5105/6700 (76%)\n",
      "\n",
      "Sung is Irish\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 7s] Train Epoch: 8 [2560/13374 (19%)]\tLoss: 200.73\n",
      "[0m 7s] Train Epoch: 8 [5120/13374 (38%)]\tLoss: 194.37\n",
      "[0m 7s] Train Epoch: 8 [7680/13374 (57%)]\tLoss: 193.64\n",
      "[0m 7s] Train Epoch: 8 [10240/13374 (77%)]\tLoss: 191.28\n",
      "[0m 7s] Train Epoch: 8 [12800/13374 (96%)]\tLoss: 190.31\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5191/6700 (77%)\n",
      "\n",
      "Sung is Irish\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 8s] Train Epoch: 9 [2560/13374 (19%)]\tLoss: 174.17\n",
      "[0m 8s] Train Epoch: 9 [5120/13374 (38%)]\tLoss: 177.03\n",
      "[0m 8s] Train Epoch: 9 [7680/13374 (57%)]\tLoss: 177.15\n",
      "[0m 8s] Train Epoch: 9 [10240/13374 (77%)]\tLoss: 176.27\n",
      "[0m 8s] Train Epoch: 9 [12800/13374 (96%)]\tLoss: 177.76\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5233/6700 (78%)\n",
      "\n",
      "Sung is Irish\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 8s] Train Epoch: 10 [2560/13374 (19%)]\tLoss: 168.54\n",
      "[0m 8s] Train Epoch: 10 [5120/13374 (38%)]\tLoss: 169.79\n",
      "[0m 9s] Train Epoch: 10 [7680/13374 (57%)]\tLoss: 167.39\n",
      "[0m 9s] Train Epoch: 10 [10240/13374 (77%)]\tLoss: 167.26\n",
      "[0m 9s] Train Epoch: 10 [12800/13374 (96%)]\tLoss: 166.11\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5300/6700 (79%)\n",
      "\n",
      "Sung is Irish\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 9s] Train Epoch: 11 [2560/13374 (19%)]\tLoss: 148.57\n",
      "[0m 9s] Train Epoch: 11 [5120/13374 (38%)]\tLoss: 148.71\n",
      "[0m 9s] Train Epoch: 11 [7680/13374 (57%)]\tLoss: 154.72\n",
      "[0m 9s] Train Epoch: 11 [10240/13374 (77%)]\tLoss: 155.77\n",
      "[0m 10s] Train Epoch: 11 [12800/13374 (96%)]\tLoss: 157.92\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5347/6700 (79%)\n",
      "\n",
      "Sung is Irish\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 10s] Train Epoch: 12 [2560/13374 (19%)]\tLoss: 143.76\n",
      "[0m 10s] Train Epoch: 12 [5120/13374 (38%)]\tLoss: 147.90\n",
      "[0m 10s] Train Epoch: 12 [7680/13374 (57%)]\tLoss: 149.50\n",
      "[0m 10s] Train Epoch: 12 [10240/13374 (77%)]\tLoss: 149.52\n",
      "[0m 10s] Train Epoch: 12 [12800/13374 (96%)]\tLoss: 149.72\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5362/6700 (80%)\n",
      "\n",
      "Sung is German\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 11s] Train Epoch: 13 [2560/13374 (19%)]\tLoss: 141.92\n",
      "[0m 11s] Train Epoch: 13 [5120/13374 (38%)]\tLoss: 138.15\n",
      "[0m 11s] Train Epoch: 13 [7680/13374 (57%)]\tLoss: 137.22\n",
      "[0m 11s] Train Epoch: 13 [10240/13374 (77%)]\tLoss: 140.43\n",
      "[0m 11s] Train Epoch: 13 [12800/13374 (96%)]\tLoss: 143.70\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5347/6700 (79%)\n",
      "\n",
      "Sung is German\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 12s] Train Epoch: 14 [2560/13374 (19%)]\tLoss: 140.12\n",
      "[0m 12s] Train Epoch: 14 [5120/13374 (38%)]\tLoss: 141.27\n",
      "[0m 12s] Train Epoch: 14 [7680/13374 (57%)]\tLoss: 138.39\n",
      "[0m 12s] Train Epoch: 14 [10240/13374 (77%)]\tLoss: 138.17\n",
      "[0m 12s] Train Epoch: 14 [12800/13374 (96%)]\tLoss: 138.63\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5360/6700 (80%)\n",
      "\n",
      "Sung is Irish\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 12s] Train Epoch: 15 [2560/13374 (19%)]\tLoss: 136.42\n",
      "[0m 12s] Train Epoch: 15 [5120/13374 (38%)]\tLoss: 134.37\n",
      "[0m 13s] Train Epoch: 15 [7680/13374 (57%)]\tLoss: 131.52\n",
      "[0m 13s] Train Epoch: 15 [10240/13374 (77%)]\tLoss: 130.73\n",
      "[0m 13s] Train Epoch: 15 [12800/13374 (96%)]\tLoss: 131.08\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5376/6700 (80%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 13s] Train Epoch: 16 [2560/13374 (19%)]\tLoss: 128.42\n",
      "[0m 13s] Train Epoch: 16 [5120/13374 (38%)]\tLoss: 128.79\n",
      "[0m 13s] Train Epoch: 16 [7680/13374 (57%)]\tLoss: 126.40\n",
      "[0m 13s] Train Epoch: 16 [10240/13374 (77%)]\tLoss: 125.96\n",
      "[0m 14s] Train Epoch: 16 [12800/13374 (96%)]\tLoss: 126.68\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5395/6700 (80%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 14s] Train Epoch: 17 [2560/13374 (19%)]\tLoss: 120.49\n",
      "[0m 14s] Train Epoch: 17 [5120/13374 (38%)]\tLoss: 121.18\n",
      "[0m 14s] Train Epoch: 17 [7680/13374 (57%)]\tLoss: 121.82\n",
      "[0m 14s] Train Epoch: 17 [10240/13374 (77%)]\tLoss: 121.59\n",
      "[0m 14s] Train Epoch: 17 [12800/13374 (96%)]\tLoss: 121.57\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5426/6700 (80%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 15s] Train Epoch: 18 [2560/13374 (19%)]\tLoss: 105.93\n",
      "[0m 15s] Train Epoch: 18 [5120/13374 (38%)]\tLoss: 111.61\n",
      "[0m 15s] Train Epoch: 18 [7680/13374 (57%)]\tLoss: 113.60\n",
      "[0m 15s] Train Epoch: 18 [10240/13374 (77%)]\tLoss: 116.63\n",
      "[0m 15s] Train Epoch: 18 [12800/13374 (96%)]\tLoss: 115.90\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5445/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 15s] Train Epoch: 19 [2560/13374 (19%)]\tLoss: 111.90\n",
      "[0m 15s] Train Epoch: 19 [5120/13374 (38%)]\tLoss: 110.49\n",
      "[0m 16s] Train Epoch: 19 [7680/13374 (57%)]\tLoss: 109.24\n",
      "[0m 16s] Train Epoch: 19 [10240/13374 (77%)]\tLoss: 109.68\n",
      "[0m 16s] Train Epoch: 19 [12800/13374 (96%)]\tLoss: 111.15\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5449/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 16s] Train Epoch: 20 [2560/13374 (19%)]\tLoss: 110.11\n",
      "[0m 16s] Train Epoch: 20 [5120/13374 (38%)]\tLoss: 108.92\n",
      "[0m 17s] Train Epoch: 20 [7680/13374 (57%)]\tLoss: 109.64\n",
      "[0m 17s] Train Epoch: 20 [10240/13374 (77%)]\tLoss: 108.53\n",
      "[0m 17s] Train Epoch: 20 [12800/13374 (96%)]\tLoss: 108.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5438/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 17s] Train Epoch: 21 [2560/13374 (19%)]\tLoss: 96.87\n",
      "[0m 17s] Train Epoch: 21 [5120/13374 (38%)]\tLoss: 101.78\n",
      "[0m 17s] Train Epoch: 21 [7680/13374 (57%)]\tLoss: 103.55\n",
      "[0m 17s] Train Epoch: 21 [10240/13374 (77%)]\tLoss: 104.55\n",
      "[0m 17s] Train Epoch: 21 [12800/13374 (96%)]\tLoss: 104.55\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5455/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 18s] Train Epoch: 22 [2560/13374 (19%)]\tLoss: 99.25\n",
      "[0m 18s] Train Epoch: 22 [5120/13374 (38%)]\tLoss: 100.25\n",
      "[0m 18s] Train Epoch: 22 [7680/13374 (57%)]\tLoss: 98.06\n",
      "[0m 18s] Train Epoch: 22 [10240/13374 (77%)]\tLoss: 98.12\n",
      "[0m 18s] Train Epoch: 22 [12800/13374 (96%)]\tLoss: 98.47\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5486/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 18s] Train Epoch: 23 [2560/13374 (19%)]\tLoss: 96.36\n",
      "[0m 19s] Train Epoch: 23 [5120/13374 (38%)]\tLoss: 97.91\n",
      "[0m 19s] Train Epoch: 23 [7680/13374 (57%)]\tLoss: 98.03\n",
      "[0m 19s] Train Epoch: 23 [10240/13374 (77%)]\tLoss: 97.22\n",
      "[0m 19s] Train Epoch: 23 [12800/13374 (96%)]\tLoss: 96.66\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5463/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 19s] Train Epoch: 24 [2560/13374 (19%)]\tLoss: 88.92\n",
      "[0m 19s] Train Epoch: 24 [5120/13374 (38%)]\tLoss: 90.76\n",
      "[0m 19s] Train Epoch: 24 [7680/13374 (57%)]\tLoss: 93.81\n",
      "[0m 20s] Train Epoch: 24 [10240/13374 (77%)]\tLoss: 93.28\n",
      "[0m 20s] Train Epoch: 24 [12800/13374 (96%)]\tLoss: 93.94\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5476/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 20s] Train Epoch: 25 [2560/13374 (19%)]\tLoss: 94.17\n",
      "[0m 20s] Train Epoch: 25 [5120/13374 (38%)]\tLoss: 89.10\n",
      "[0m 20s] Train Epoch: 25 [7680/13374 (57%)]\tLoss: 87.38\n",
      "[0m 20s] Train Epoch: 25 [10240/13374 (77%)]\tLoss: 88.88\n",
      "[0m 20s] Train Epoch: 25 [12800/13374 (96%)]\tLoss: 88.87\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5469/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 21s] Train Epoch: 26 [2560/13374 (19%)]\tLoss: 91.56\n",
      "[0m 21s] Train Epoch: 26 [5120/13374 (38%)]\tLoss: 88.30\n",
      "[0m 21s] Train Epoch: 26 [7680/13374 (57%)]\tLoss: 88.06\n",
      "[0m 21s] Train Epoch: 26 [10240/13374 (77%)]\tLoss: 85.65\n",
      "[0m 21s] Train Epoch: 26 [12800/13374 (96%)]\tLoss: 84.98\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5484/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 22s] Train Epoch: 27 [2560/13374 (19%)]\tLoss: 78.92\n",
      "[0m 22s] Train Epoch: 27 [5120/13374 (38%)]\tLoss: 78.87\n",
      "[0m 22s] Train Epoch: 27 [7680/13374 (57%)]\tLoss: 80.71\n",
      "[0m 22s] Train Epoch: 27 [10240/13374 (77%)]\tLoss: 82.47\n",
      "[0m 22s] Train Epoch: 27 [12800/13374 (96%)]\tLoss: 83.00\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5486/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 22s] Train Epoch: 28 [2560/13374 (19%)]\tLoss: 79.17\n",
      "[0m 22s] Train Epoch: 28 [5120/13374 (38%)]\tLoss: 82.60\n",
      "[0m 22s] Train Epoch: 28 [7680/13374 (57%)]\tLoss: 81.63\n",
      "[0m 23s] Train Epoch: 28 [10240/13374 (77%)]\tLoss: 81.04\n",
      "[0m 23s] Train Epoch: 28 [12800/13374 (96%)]\tLoss: 82.13\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5449/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 23s] Train Epoch: 29 [2560/13374 (19%)]\tLoss: 79.19\n",
      "[0m 23s] Train Epoch: 29 [5120/13374 (38%)]\tLoss: 82.68\n",
      "[0m 23s] Train Epoch: 29 [7680/13374 (57%)]\tLoss: 83.99\n",
      "[0m 23s] Train Epoch: 29 [10240/13374 (77%)]\tLoss: 83.79\n",
      "[0m 23s] Train Epoch: 29 [12800/13374 (96%)]\tLoss: 83.03\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5450/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 24s] Train Epoch: 30 [2560/13374 (19%)]\tLoss: 75.30\n",
      "[0m 24s] Train Epoch: 30 [5120/13374 (38%)]\tLoss: 73.21\n",
      "[0m 24s] Train Epoch: 30 [7680/13374 (57%)]\tLoss: 73.26\n",
      "[0m 24s] Train Epoch: 30 [10240/13374 (77%)]\tLoss: 73.64\n",
      "[0m 24s] Train Epoch: 30 [12800/13374 (96%)]\tLoss: 75.78\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5462/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 24s] Train Epoch: 31 [2560/13374 (19%)]\tLoss: 70.46\n",
      "[0m 25s] Train Epoch: 31 [5120/13374 (38%)]\tLoss: 70.40\n",
      "[0m 25s] Train Epoch: 31 [7680/13374 (57%)]\tLoss: 70.97\n",
      "[0m 25s] Train Epoch: 31 [10240/13374 (77%)]\tLoss: 72.54\n",
      "[0m 25s] Train Epoch: 31 [12800/13374 (96%)]\tLoss: 71.77\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5468/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 25s] Train Epoch: 32 [2560/13374 (19%)]\tLoss: 69.00\n",
      "[0m 25s] Train Epoch: 32 [5120/13374 (38%)]\tLoss: 66.30\n",
      "[0m 25s] Train Epoch: 32 [7680/13374 (57%)]\tLoss: 67.17\n",
      "[0m 26s] Train Epoch: 32 [10240/13374 (77%)]\tLoss: 68.35\n",
      "[0m 26s] Train Epoch: 32 [12800/13374 (96%)]\tLoss: 69.42\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5488/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 26s] Train Epoch: 33 [2560/13374 (19%)]\tLoss: 62.66\n",
      "[0m 26s] Train Epoch: 33 [5120/13374 (38%)]\tLoss: 64.10\n",
      "[0m 26s] Train Epoch: 33 [7680/13374 (57%)]\tLoss: 64.12\n",
      "[0m 26s] Train Epoch: 33 [10240/13374 (77%)]\tLoss: 66.46\n",
      "[0m 26s] Train Epoch: 33 [12800/13374 (96%)]\tLoss: 67.47\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5475/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 27s] Train Epoch: 34 [2560/13374 (19%)]\tLoss: 61.65\n",
      "[0m 27s] Train Epoch: 34 [5120/13374 (38%)]\tLoss: 63.45\n",
      "[0m 27s] Train Epoch: 34 [7680/13374 (57%)]\tLoss: 61.66\n",
      "[0m 27s] Train Epoch: 34 [10240/13374 (77%)]\tLoss: 62.29\n",
      "[0m 27s] Train Epoch: 34 [12800/13374 (96%)]\tLoss: 63.72\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5483/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 27s] Train Epoch: 35 [2560/13374 (19%)]\tLoss: 57.52\n",
      "[0m 28s] Train Epoch: 35 [5120/13374 (38%)]\tLoss: 58.80\n",
      "[0m 28s] Train Epoch: 35 [7680/13374 (57%)]\tLoss: 61.62\n",
      "[0m 28s] Train Epoch: 35 [10240/13374 (77%)]\tLoss: 63.24\n",
      "[0m 28s] Train Epoch: 35 [12800/13374 (96%)]\tLoss: 64.63\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5451/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 28s] Train Epoch: 36 [2560/13374 (19%)]\tLoss: 57.56\n",
      "[0m 28s] Train Epoch: 36 [5120/13374 (38%)]\tLoss: 61.12\n",
      "[0m 28s] Train Epoch: 36 [7680/13374 (57%)]\tLoss: 61.29\n",
      "[0m 29s] Train Epoch: 36 [10240/13374 (77%)]\tLoss: 61.87\n",
      "[0m 29s] Train Epoch: 36 [12800/13374 (96%)]\tLoss: 61.82\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5459/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 29s] Train Epoch: 37 [2560/13374 (19%)]\tLoss: 60.43\n",
      "[0m 29s] Train Epoch: 37 [5120/13374 (38%)]\tLoss: 57.48\n",
      "[0m 29s] Train Epoch: 37 [7680/13374 (57%)]\tLoss: 58.16\n",
      "[0m 29s] Train Epoch: 37 [10240/13374 (77%)]\tLoss: 58.49\n",
      "[0m 29s] Train Epoch: 37 [12800/13374 (96%)]\tLoss: 59.88\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5422/6700 (80%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 30s] Train Epoch: 38 [2560/13374 (19%)]\tLoss: 56.36\n",
      "[0m 30s] Train Epoch: 38 [5120/13374 (38%)]\tLoss: 56.53\n",
      "[0m 30s] Train Epoch: 38 [7680/13374 (57%)]\tLoss: 57.34\n",
      "[0m 30s] Train Epoch: 38 [10240/13374 (77%)]\tLoss: 58.51\n",
      "[0m 30s] Train Epoch: 38 [12800/13374 (96%)]\tLoss: 59.90\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5472/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 30s] Train Epoch: 39 [2560/13374 (19%)]\tLoss: 55.90\n",
      "[0m 31s] Train Epoch: 39 [5120/13374 (38%)]\tLoss: 57.89\n",
      "[0m 31s] Train Epoch: 39 [7680/13374 (57%)]\tLoss: 57.64\n",
      "[0m 31s] Train Epoch: 39 [10240/13374 (77%)]\tLoss: 58.71\n",
      "[0m 31s] Train Epoch: 39 [12800/13374 (96%)]\tLoss: 59.86\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5480/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 31s] Train Epoch: 40 [2560/13374 (19%)]\tLoss: 56.03\n",
      "[0m 31s] Train Epoch: 40 [5120/13374 (38%)]\tLoss: 56.89\n",
      "[0m 31s] Train Epoch: 40 [7680/13374 (57%)]\tLoss: 57.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 31s] Train Epoch: 40 [10240/13374 (77%)]\tLoss: 56.97\n",
      "[0m 32s] Train Epoch: 40 [12800/13374 (96%)]\tLoss: 57.31\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5466/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 32s] Train Epoch: 41 [2560/13374 (19%)]\tLoss: 53.06\n",
      "[0m 32s] Train Epoch: 41 [5120/13374 (38%)]\tLoss: 51.34\n",
      "[0m 32s] Train Epoch: 41 [7680/13374 (57%)]\tLoss: 51.07\n",
      "[0m 32s] Train Epoch: 41 [10240/13374 (77%)]\tLoss: 51.56\n",
      "[0m 32s] Train Epoch: 41 [12800/13374 (96%)]\tLoss: 51.08\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5452/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 33s] Train Epoch: 42 [2560/13374 (19%)]\tLoss: 51.00\n",
      "[0m 33s] Train Epoch: 42 [5120/13374 (38%)]\tLoss: 49.54\n",
      "[0m 33s] Train Epoch: 42 [7680/13374 (57%)]\tLoss: 50.86\n",
      "[0m 33s] Train Epoch: 42 [10240/13374 (77%)]\tLoss: 50.26\n",
      "[0m 33s] Train Epoch: 42 [12800/13374 (96%)]\tLoss: 51.07\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5435/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 34s] Train Epoch: 43 [2560/13374 (19%)]\tLoss: 41.62\n",
      "[0m 34s] Train Epoch: 43 [5120/13374 (38%)]\tLoss: 43.97\n",
      "[0m 34s] Train Epoch: 43 [7680/13374 (57%)]\tLoss: 45.94\n",
      "[0m 34s] Train Epoch: 43 [10240/13374 (77%)]\tLoss: 49.12\n",
      "[0m 34s] Train Epoch: 43 [12800/13374 (96%)]\tLoss: 50.39\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5449/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 34s] Train Epoch: 44 [2560/13374 (19%)]\tLoss: 51.04\n",
      "[0m 34s] Train Epoch: 44 [5120/13374 (38%)]\tLoss: 51.95\n",
      "[0m 35s] Train Epoch: 44 [7680/13374 (57%)]\tLoss: 51.53\n",
      "[0m 35s] Train Epoch: 44 [10240/13374 (77%)]\tLoss: 51.37\n",
      "[0m 35s] Train Epoch: 44 [12800/13374 (96%)]\tLoss: 51.83\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5437/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 35s] Train Epoch: 45 [2560/13374 (19%)]\tLoss: 51.09\n",
      "[0m 35s] Train Epoch: 45 [5120/13374 (38%)]\tLoss: 52.22\n",
      "[0m 35s] Train Epoch: 45 [7680/13374 (57%)]\tLoss: 50.85\n",
      "[0m 35s] Train Epoch: 45 [10240/13374 (77%)]\tLoss: 50.93\n",
      "[0m 35s] Train Epoch: 45 [12800/13374 (96%)]\tLoss: 51.09\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5453/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 36s] Train Epoch: 46 [2560/13374 (19%)]\tLoss: 53.05\n",
      "[0m 36s] Train Epoch: 46 [5120/13374 (38%)]\tLoss: 49.29\n",
      "[0m 36s] Train Epoch: 46 [7680/13374 (57%)]\tLoss: 47.86\n",
      "[0m 36s] Train Epoch: 46 [10240/13374 (77%)]\tLoss: 48.34\n",
      "[0m 36s] Train Epoch: 46 [12800/13374 (96%)]\tLoss: 48.14\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5460/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 37s] Train Epoch: 47 [2560/13374 (19%)]\tLoss: 41.70\n",
      "[0m 37s] Train Epoch: 47 [5120/13374 (38%)]\tLoss: 43.03\n",
      "[0m 37s] Train Epoch: 47 [7680/13374 (57%)]\tLoss: 43.70\n",
      "[0m 37s] Train Epoch: 47 [10240/13374 (77%)]\tLoss: 44.23\n",
      "[0m 37s] Train Epoch: 47 [12800/13374 (96%)]\tLoss: 45.51\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5456/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 37s] Train Epoch: 48 [2560/13374 (19%)]\tLoss: 42.08\n",
      "[0m 38s] Train Epoch: 48 [5120/13374 (38%)]\tLoss: 43.53\n",
      "[0m 38s] Train Epoch: 48 [7680/13374 (57%)]\tLoss: 43.10\n",
      "[0m 38s] Train Epoch: 48 [10240/13374 (77%)]\tLoss: 43.20\n",
      "[0m 38s] Train Epoch: 48 [12800/13374 (96%)]\tLoss: 43.80\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5463/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 38s] Train Epoch: 49 [2560/13374 (19%)]\tLoss: 40.66\n",
      "[0m 38s] Train Epoch: 49 [5120/13374 (38%)]\tLoss: 41.18\n",
      "[0m 38s] Train Epoch: 49 [7680/13374 (57%)]\tLoss: 42.08\n",
      "[0m 38s] Train Epoch: 49 [10240/13374 (77%)]\tLoss: 43.18\n",
      "[0m 39s] Train Epoch: 49 [12800/13374 (96%)]\tLoss: 44.43\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5432/6700 (81%)\n",
      "\n",
      "Sung is German\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 39s] Train Epoch: 50 [2560/13374 (19%)]\tLoss: 39.69\n",
      "[0m 39s] Train Epoch: 50 [5120/13374 (38%)]\tLoss: 42.74\n",
      "[0m 39s] Train Epoch: 50 [7680/13374 (57%)]\tLoss: 41.83\n",
      "[0m 39s] Train Epoch: 50 [10240/13374 (77%)]\tLoss: 41.99\n",
      "[0m 39s] Train Epoch: 50 [12800/13374 (96%)]\tLoss: 42.65\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5415/6700 (80%)\n",
      "\n",
      "Sung is German\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 40s] Train Epoch: 51 [2560/13374 (19%)]\tLoss: 40.09\n",
      "[0m 40s] Train Epoch: 51 [5120/13374 (38%)]\tLoss: 37.83\n",
      "[0m 40s] Train Epoch: 51 [7680/13374 (57%)]\tLoss: 39.07\n",
      "[0m 40s] Train Epoch: 51 [10240/13374 (77%)]\tLoss: 39.07\n",
      "[0m 40s] Train Epoch: 51 [12800/13374 (96%)]\tLoss: 40.47\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5455/6700 (81%)\n",
      "\n",
      "Sung is German\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 40s] Train Epoch: 52 [2560/13374 (19%)]\tLoss: 38.68\n",
      "[0m 41s] Train Epoch: 52 [5120/13374 (38%)]\tLoss: 40.10\n",
      "[0m 41s] Train Epoch: 52 [7680/13374 (57%)]\tLoss: 40.66\n",
      "[0m 41s] Train Epoch: 52 [10240/13374 (77%)]\tLoss: 40.60\n",
      "[0m 41s] Train Epoch: 52 [12800/13374 (96%)]\tLoss: 42.06\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5430/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 41s] Train Epoch: 53 [2560/13374 (19%)]\tLoss: 38.19\n",
      "[0m 41s] Train Epoch: 53 [5120/13374 (38%)]\tLoss: 38.74\n",
      "[0m 41s] Train Epoch: 53 [7680/13374 (57%)]\tLoss: 38.33\n",
      "[0m 42s] Train Epoch: 53 [10240/13374 (77%)]\tLoss: 38.40\n",
      "[0m 42s] Train Epoch: 53 [12800/13374 (96%)]\tLoss: 38.81\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5463/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is English\n",
      "Soojin is German\n",
      "Nako is Russian\n",
      "[0m 42s] Train Epoch: 54 [2560/13374 (19%)]\tLoss: 33.81\n",
      "[0m 42s] Train Epoch: 54 [5120/13374 (38%)]\tLoss: 34.26\n",
      "[0m 42s] Train Epoch: 54 [7680/13374 (57%)]\tLoss: 35.53\n",
      "[0m 42s] Train Epoch: 54 [10240/13374 (77%)]\tLoss: 36.93\n",
      "[0m 42s] Train Epoch: 54 [12800/13374 (96%)]\tLoss: 37.20\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5445/6700 (81%)\n",
      "\n",
      "Sung is Dutch\n",
      "Jungwoo is Russian\n",
      "Soojin is German\n",
      "Nako is Russian\n",
      "[0m 43s] Train Epoch: 55 [2560/13374 (19%)]\tLoss: 36.10\n",
      "[0m 43s] Train Epoch: 55 [5120/13374 (38%)]\tLoss: 37.98\n",
      "[0m 43s] Train Epoch: 55 [7680/13374 (57%)]\tLoss: 38.02\n",
      "[0m 43s] Train Epoch: 55 [10240/13374 (77%)]\tLoss: 38.62\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-35a76094a7f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Train cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-152-5ec17099224f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountries\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#         print(input.shape, output.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-153-3ec3e0d5dfd5>\u001b[0m in \u001b[0;36mmake_variables\u001b[1;34m(names, countries)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mvectorized_seqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequence_and_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mseq_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequence_and_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorized_seqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-153-3ec3e0d5dfd5>\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(vectorized_seqs, seq_lengths, countries)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcountries2tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Return variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    classifier.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    # Train cycle\n",
    "    train()\n",
    "\n",
    "    # Testing\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
